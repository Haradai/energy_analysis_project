{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting jsons file from the sketchy website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weather_data = pd.DataFrame(columns=[\"date\"])\n",
    "for month in range(1,12+1):\n",
    "    with open(f'data/jsons_weather_monthly/{str(month).zfill(2)}.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for daily in data[\"data\"][\"weather\"]:\n",
    "        hourly_data = []\n",
    "        for hourly in daily[\"hourly\"]:\n",
    "            hourly_data.append(pd.DataFrame.from_dict(hourly))\n",
    "        daily_data = pd.concat(hourly_data)\n",
    "        daily_data[\"date\"] = daily[\"date\"]\n",
    "        weather_data = pd.concat([weather_data, daily_data])\n",
    "\n",
    "weather_data.to_csv(\"data/weather_hourly.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the datasets in one csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter interesting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data[['date', 'time', 'tempC','windspeedKmph','weatherCode','precipMM','humidity','pressure','cloudcover','WindChillC','WindGustKmph','FeelsLikeC','uvIndex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read weather data\n",
    "weather_data = {}\n",
    "\n",
    "with open(\"data/weather_hourly.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        date = row[\"date\"]\n",
    "        time = row[\"time\"]\n",
    "        hour = int(time) // 100\n",
    "        dt = datetime.datetime.strptime(date, \"%Y-%m-%d\").replace(hour=hour)\n",
    "        weather_data[dt] = row\n",
    "\"\"\"\n",
    "# Read and encode occupation data\n",
    "occupation_df = pd.read_csv(\"data/ocupacio_enginyeria_2022.csv\")\n",
    "columns_to_encode = ['Espai', 'Estudi', 'Activitat', 'Modalitat docencia']\n",
    "\n",
    "#for col in columns_to_encode:\n",
    "#    label_encoder = LabelEncoder()\n",
    "#    occupation_df[col] = label_encoder.fit_transform(occupation_df[col].astype(str))\n",
    "\n",
    "occupation_data = {}\n",
    "\n",
    "for _, row in occupation_df.iterrows():\n",
    "    if not row[\"Data inicial\"].strip():  # Check if the date string is empty\n",
    "        continue\n",
    "    start_date = datetime.datetime.strptime(row[\"Data inicial\"], \"%d/%m/%Y\")\n",
    "    start_hour = int(row[\"Hora inicial\"].split(\":\")[0])\n",
    "    start_dt = start_date.replace(hour=start_hour)\n",
    "    occupation_data[start_dt] = row.to_dict()\n",
    "\n",
    "\"\"\"\n",
    "# Read target data\n",
    "target_data = {}\n",
    "\n",
    "with open(\"data/Consum horari electricitat Enginyeries 2022.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\";\")\n",
    "    for row in reader:\n",
    "        date = datetime.datetime.strptime(row[\"Date\"], \"%d/%m/%Y\")\n",
    "        hour = int(row[\"Hour\"].split(\":\")[0])\n",
    "        dt = date.replace(hour=hour)\n",
    "        target_data[dt] = row\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for dt in sorted(weather_data.keys()):\n",
    "    merged_row = weather_data[dt]\n",
    "    \"\"\"\n",
    "    if dt in occupation_data:\n",
    "        merged_row.update(occupation_data[dt])\n",
    "    \"\"\"\n",
    "    if dt in target_data:\n",
    "        merged_row.update(target_data[dt])\n",
    "\n",
    "    merged_data.append(merged_row)\n",
    "\n",
    "# Save the merged data to a CSV file\n",
    "with open(\"data/merged_data.csv\", \"w\", newline=\"\") as f:\n",
    "    fieldnames = set()\n",
    "    for row in merged_data:\n",
    "        fieldnames.update(row.keys())\n",
    "    fieldnames = list(fieldnames)\n",
    "\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in merged_data:\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"data/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.read_csv(\"data/merged_data.csv\")\n",
    "\n",
    "#columns_to_drop = ['Unnamed: 0', 'Data inicial', 'Data final', 'Hora inicial', 'Hora final', 'time', 'weatherIconUrl', 'winddir16Point', 'Date', 'Hour', 'Observacions', 'tempF', 'WindChillF', 'HeatIndexF', 'FeelsLikeF', 'DewPointF', 'WindGustMiles', 'visibilityMiles', 'pressureInches', 'precipInches']\n",
    "columns_to_drop = ['Unnamed: 0', 'weatherIconUrl', 'winddir16Point', 'Date', 'Hour', 'tempF', 'WindChillF', 'weatherDesc','HeatIndexF', 'FeelsLikeF', 'DewPointF', 'WindGustMiles', 'visibilityMiles', 'pressureInches', 'precipInches']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "merged_df[\"time\"] = merged_df[\"time\"]/100\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "coloms_with_coma_decimal = [\"Q-Enginyeria (Cos Central) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Espina 4) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Qu√≠mica) [kWh] [Q-Enginyeria]\"]\n",
    "for col in coloms_with_coma_decimal:\n",
    "    changed = []\n",
    "    for i,val in enumerate(merged_df[col]):\n",
    "        if val == val:\n",
    "            changed.append(val.replace(\",\",\".\"))\n",
    "        else: #we got a nan. interpolate by having previous value\n",
    "            changed.append(changed[-1])\n",
    "    \n",
    "    merged_df[col] = changed\n",
    "\n",
    "merged_df.to_csv(\"data/updated_merged_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"data/updated_merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the future we could play with [observations, Estudi,Modalitat docencia] columns and NLP to get some representation of the activity being done and its impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_data = pd.read_csv(\"data/ocupacio_enginyeria_2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and embedding for the title of activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we have is that we have too many activities to create a one hot vector of each of them.\n",
    "Second problem we have is that if we wanted to add a new activity this would break the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All activities we have.\n",
    "occupation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future it would be cool to create our own model that creates some embedding optimized for owr problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we'll use some pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"[CLS] \"\n",
    "end_token = \" [SEP]\"\n",
    "test_txt = start_token + \"This class is chaotic and boring at the same time. \" + end_token\n",
    "tokenized_text = tokenizer.tokenize(test_txt)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [1] * len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).to(\"mps\")\n",
    "segments_tensors = torch.tensor([segments_ids]).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `token_vecs` is a tensor with sha pe [Ntokens x 768]\n",
    "token_vecs = hidden_states[-2][0] #second to last hiden layer\n",
    "\n",
    "# Calculate the average of all Ntokens token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2hidden_states(txt):\n",
    "    test_txt = start_token + txt + end_token\n",
    "    tokenized_text = tokenizer.tokenize(test_txt)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(\"mps\")\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(\"mps\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    hidden_states = outputs[2]\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an embedding for each sentence of size (768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to apply kmeans to kluster all titles by their semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "h_states = {}\n",
    "occupations = set()\n",
    "for occu in tqdm(occupation_data[\"Activitat\"]):\n",
    "    if occu not in occupations:\n",
    "        h = sentence2hidden_states(occu)[-2][0].to(\"cpu\") #get all layers first batch\n",
    "        h_states[occu] =h\n",
    "\n",
    "        # Calculate the average of all Ntokens token vectors.\n",
    "        sentence_embedding = torch.mean(h, dim=0)\n",
    "        vectors[occu] = sentence_embedding.numpy()\n",
    "        occupations.add(occu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same but encode the classroom the activity is in the text also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_plus = {}\n",
    "h_states_plus = {}\n",
    "occupations_plus = set()\n",
    "for occu, espai in zip(occupation_data[\"Activitat\"],occupation_data[\"Espai\"]):\n",
    "    if occu + \" \" + espai not in occupations_plus:\n",
    "        h = sentence2hidden_states(occu + \" \" + espai)[-2][0].to(\"cpu\") #get all layers first batch\n",
    "        h_states_plus[occu + \" \" + espai] = h\n",
    "        \n",
    "        # Calculate the average of all Ntokens token vectors.\n",
    "        sentence_embedding = torch.mean(h, dim=0)\n",
    "        vectors_plus[occu + \" \" + espai] = sentence_embedding.numpy()\n",
    "        occupations_plus.add(occu + \" \" + espai) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to pickle file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#bert_data = {\n",
    "#    \"single_ocu\":{\"mean_vect\":vectors,\"h_states\":h_states},\n",
    "#    \"ocu_plus_space\":{\"mean_vect\":vectors_plus,\"h_states\":h_states_plus}\n",
    "#    }\n",
    "\n",
    "bert_data = {\n",
    "    \"single_ocu\":{\"mean_vect\":vectors},\n",
    "    \"ocu_plus_space\":{\"mean_vect\":vectors_plus}\n",
    "    }\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open('data/bert_embedded.pkl', 'wb') as file:\n",
    "      \n",
    "    pickle.dump(bert_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do some fast clustering to see if it makes any sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(list(vectors_plus.values()))\n",
    "cluster_cent = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = pd.DataFrame()\n",
    "cluster_map['data_index'] = list(vectors_plus.keys())\n",
    "cluster_map['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map[cluster_map[\"cluster\"]==0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map[cluster_map[\"cluster\"]==1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map[cluster_map[\"cluster\"]==2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map[cluster_map[\"cluster\"]==3].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll not sure if it makes any sense XD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add to the dataset a column for the hiddent states of bert. We'll pass a text through bert if the input class doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class energyProject_dataset(Dataset):\n",
    "    def __init__(self,dataset_pth,occupacio_pth,bert_embeddings_pkl_pth,pca_pkl_pth=None):\n",
    "        self.df = pd.read_csv(dataset_pth)\n",
    "        with (open(bert_embeddings_pkl_pth, \"rb\")) as openfile:\n",
    "            self.bert_embeddings = pickle.load(openfile)\n",
    "        self.activitivity_encoding_mode = 0\n",
    "        \n",
    "        #order of values in target tensor will follow this\n",
    "        self.target_labels = [\"Q-Enginyeria (Cos Central) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Espina 4) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Qu√≠mica) [kWh] [Q-Enginyeria]\"]\n",
    "       \n",
    "        #load occupation data\n",
    "        self.occupation_df = pd.read_csv(occupacio_pth)\n",
    "        #we'll remove entries without date\n",
    "        self.occupation_df = self.occupation_df[self.occupation_df[\"Data inicial\"] != \" \"]\n",
    "\n",
    "        #convert date string to be in the form y-m-d instead of d/m/y\n",
    "        #convert hour data to datetime object so we can compare them\n",
    "        for i, row in self.occupation_df.iterrows(): \n",
    "            self.occupation_df.loc[i][\"Data inicial\"] =  datetime.datetime.strptime(self.occupation_df.loc[i][\"Data inicial\"], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            self.occupation_df.loc[i][\"Hora inicial\"] = datetime.datetime.strptime(self.occupation_df.loc[i][\"Hora inicial\"] ,\"%H:%M\").time()\n",
    "            self.occupation_df.loc[i][\"Hora final\"] = datetime.datetime.strptime(self.occupation_df.loc[i][\"Hora final\"] ,\"%H:%M\").time()\n",
    "        \n",
    "        self.ocup_vocab = list(set(self.occupation_df[\"Activitat\"]))\n",
    "        self.espais_vocab = list(set(self.occupation_df[\"Espai\"]))\n",
    "        #Add a padding occupation\n",
    "        self.espais_vocab.append(\"NO ESPAI\")\n",
    "\n",
    "        #Normalize all climate data to be between 0-1\n",
    "        #we'll keep their scaler objects so we can transform their values back to original\n",
    "        #and not loose meaning.\n",
    "        self.column_scalers = {}\n",
    "        columns_to_process = ['winddirDegree', 'precipMM', 'visibility', 'WindChillC',\n",
    "       'humidity', 'pressure','windspeedMiles', 'uvIndex', 'DewPointC',\n",
    "       'FeelsLikeC', 'tempC','weatherCode','HeatIndexC', 'WindGustKmph', 'cloudcover',\n",
    "       'windspeedKmph','Q-Enginyeria (Cos Central) [kWh] [Q-Enginyeria]',\n",
    "       'Q-Enginyeria (Qu√≠mica) [kWh] [Q-Enginyeria]',\n",
    "       'Q-Enginyeria (Espina 4) [kWh] [Q-Enginyeria]'] #normalze also target\n",
    "     \n",
    "        for col in columns_to_process:\n",
    "            scaler, values = self.normalize_values(self.df[col])\n",
    "            self.df[col] = values\n",
    "            self.column_scalers[col] = scaler\n",
    "        \n",
    "        #If we want this dataset to work with batches in modes different than 0 and 1.\n",
    "        #We need to know what is the maximum number of activities at the same time so we can\n",
    "        #padd the samples smaller. We need the batch to have the same shape samples every time.\n",
    "\n",
    "        #we'll use that we are iterating throught this to compute all the one_hot vectors of the encodings\n",
    "        #and fit a PCA object so we can have it with dimensionality reduction.\n",
    "\n",
    "        self.max_ocu_lenght = 0\n",
    "        self.all_one_hots = []\n",
    "        for i, row in self.df.iterrows():\n",
    "            day2day_ocu = self.activty_class_perT(row[\"date\"],row[\"time\"])\n",
    "            \n",
    "            #find largest occupation size per hour\n",
    "            day2day_ocu_l = len(day2day_ocu)\n",
    "            if day2day_ocu_l > self.max_ocu_lenght:\n",
    "                self.max_ocu_lenght = day2day_ocu_l\n",
    "                \n",
    "            if pca_pkl_pth == True: #if we have to calculate the PCA\n",
    "                #compute one_hot vectors of occupation at this time\n",
    "                self.all_one_hots.append(self.activity_class_one_hot(day2day_ocu))\n",
    "        \n",
    "        if (pca_pkl_pth == True): #nan value, recalculate and save file\n",
    "            self.all_one_hots = np.array(self.all_one_hots)\n",
    "            #compute PCA on all the one_hot vectors\n",
    "            self.ocu_one_hot_pca = PCA(n_components=1000) #1000 size output vector (number chosen by hand)\n",
    "            self.ocu_one_hot_pca.fit(self.all_one_hots)\n",
    "            # Open a file and use dump()\n",
    "            with open('data/pca_occupation.pkl', 'wb') as file:\n",
    "                pickle.dump(self.ocu_one_hot_pca, file)\n",
    "        else:\n",
    "            with (open(pca_pkl_pth, \"rb\")) as openfile:\n",
    "                self.ocu_one_hot_pca = pickle.load(openfile)\n",
    "        \n",
    "        ##free memory by removing unnecessary variables.\n",
    "        del self.all_one_hots\n",
    "\n",
    "    def normalize_values(self,x):\n",
    "        \"\"\"\n",
    "        Input a list of values\n",
    "        Output a sklearn scaler object and the list normalized.\n",
    "        We need to keep the scaler to be able to re-scale the data back and now what value it is in reality.\n",
    "        \"\"\"\n",
    "        to_scale = np.array(x).reshape(-1, 1) #the library needs this extra dimensions trick to interpret properly\n",
    "        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "        x_scaled = min_max_scaler.fit_transform(to_scale)\n",
    "        return min_max_scaler, x_scaled\n",
    "    \n",
    "    def denormalize_values(self,x_n,scaler):\n",
    "        \"\"\"\n",
    "        Given some set of values and a sklearn scaler object\n",
    "        Transform back the values to their original \"space\"ArithmeticError\n",
    "        return: set of values same shape as input\n",
    "        \"\"\"\n",
    "        to_scale = np.array(x_n).reshape(-1, 1)\n",
    "        return scaler.inverse_transform(to_scale)\n",
    "\n",
    "    def datetime_enc(self,date, time)->torch.tensor:\n",
    "        \"\"\"\n",
    "        Encodes incoming date and time strings as two values for each that \n",
    "        come from infering the index value on a sin function and cos function.\n",
    "        Its nice beacause we encode the smoothness and circularity of the trigonometric\n",
    "        functions.\n",
    "\n",
    "        input <- (date: str, time:str)\n",
    "        output -> (torch.tensor((1,5)))\n",
    "        \"\"\"\n",
    "        \n",
    "        date_obj =  datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        \n",
    "        #encode year as floas\n",
    "        year_enc = float(date_obj.year)/1000 # divide by 100 to have reasonable value\n",
    "        \n",
    "        ##\n",
    "        ##Encoding: (sin, cos) value for each day month\n",
    "        ##\n",
    "        idx_d = date_obj.timetuple().tm_yday #day of the year number\n",
    "        date_enc= [np.sin((idx_d/365) * 2*np.pi),np.cos((idx_d/365) * 2*np.pi)]  #365 days a yar +1 offset so we don't have negative value\n",
    "        \n",
    "        #encode time by hour in the day\n",
    "        time_enc = [np.sin((time/24) * 2*np.pi),np.cos((time/24) * 2*np.pi)]  #24 hours a day. +1 offset so we don't have negative values\n",
    "        \n",
    "        return torch.tensor([year_enc]+date_enc+time_enc) \n",
    "\n",
    "    def datetime_dec(self,enc_tens):\n",
    "        \"\"\"\n",
    "        Decodes incoming encoded date and time tensor as the two respective\n",
    "        date time string values\n",
    "\n",
    "        input <- (torch.tensor([torch.float,torch.float]))\n",
    "        output -> date: str, time:str\n",
    "        \"\"\"\n",
    "        #decode year\n",
    "        year = int(enc_tens[0].item() * 1000)\n",
    "\n",
    "        #decode date\n",
    "        penc_date = np.arctan2(enc_tens[1].item(),enc_tens[2].item()) / (2*np.pi) * 365\n",
    "        date = datetime.datetime(year, 1, 1) + datetime.timedelta(penc_date - 1)\n",
    "        date = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        #decode time\n",
    "        time = np.round((np.arctan2(enc_tens[3].item(),enc_tens[4].item()) / (2*np.pi) * 24)% 24)\n",
    "        \n",
    "        return date,time\n",
    "\n",
    "    def activty_class_perT(self,date,time)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns slice of the pd Dataframe of activities active given some date and time\n",
    "        \"\"\"\n",
    "        #filter dataset to see activities that day\n",
    "        day2day_ocu = self.occupation_df[self.occupation_df[\"Data inicial\"] == date ] \n",
    "        h = datetime.time(int(time))\n",
    "        hour2hour_ocu = day2day_ocu[(day2day_ocu[\"Hora inicial\"] <= h) & (day2day_ocu[\"Hora final\"] > h)]\n",
    "        return pd.DataFrame(hour2hour_ocu)\n",
    "\n",
    "    def activity_class_one_hot(self,activities)->np.array:\n",
    "        \"\"\" \n",
    "        returns flattenned coocurrence one-hot matrix of activities and classrooms\n",
    "        \"\"\"\n",
    "        occurrence_matrix = np.zeros((len(self.ocup_vocab),len(self.espais_vocab)))\n",
    "        for i,actv in activities.iterrows(): #iterate found activities\n",
    "            ocup_idx = self.ocup_vocab.index(actv[\"Activitat\"])\n",
    "            espais_idx = self.espais_vocab.index(actv[\"Espai\"])\n",
    "            occurrence_matrix[ocup_idx,espais_idx] = 1\n",
    "            \n",
    "        #now flatten the occurrence matrix into a one hot vector\n",
    "        one_hot = occurrence_matrix.flatten()\n",
    "        return one_hot\n",
    "    \n",
    "    def class_one_hot(self,activitats)->torch.tensor:\n",
    "        \"\"\"\n",
    "        Returns \"one hot\" encoding of activitat.\n",
    "        In reality will not be a true one hot but a list of indexes\n",
    "        that can later on be passed to some embedding layer\n",
    "        \"\"\"\n",
    "        activitats[\"Espai\"]\n",
    "        one_hot_esp = [self.espais_vocab.index(key) for key in activitats[\"Espai\"]]\n",
    "        return torch.tensor(one_hot_esp)\n",
    "       \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This function will return more than one object depending on the mode it is on.\n",
    "        Activity encoding mode:\n",
    "            0: I.very large one-hot encoding of all the combinations of classroom and activity concatenated\n",
    "                with all other features.\n",
    "               II. target values\n",
    "            \n",
    "            1:  I. 0 but first with some PCA applied to reduce dimensionality of \n",
    "                the enormous one-hot encoding.\n",
    "                II. target values\n",
    "            \n",
    "            2: returns four objects, \n",
    "                I.mean_encoding of activity from bert(as many as activities at time stamp), \n",
    "                II. classroom one hot encoding for each activity(as many as activities at time stamp)\n",
    "                III. All other features at that time stamp\n",
    "                IV. target values\n",
    "            \n",
    "            2.5:  returns four objects\n",
    "                I. embedding tensor of activity from bert(as many as activities at time stamp), \n",
    "                II. classroom one hot encoding for each activity(as many as activities at time stamp)\n",
    "                III. All other features at that time stamp\n",
    "                IV. target values\n",
    "            \n",
    "            3: returns three objects,\n",
    "                I. mean encoding of activity, classroom pair through bert (as many as activities at time stamp),\n",
    "                II. All other features at that time stamp\n",
    "                III. target values\n",
    "            \n",
    "            3.5:  returns four objects\n",
    "                I. embedding tensor of activity, classroom pair through bert(as many as activities at time stamp), \n",
    "                II. classroom one hot encoding for each activity(as many as activities at time stamp)\n",
    "                III. All other features at that time stamp\n",
    "                IV. target values\n",
    "            \n",
    "            4: returns three objects, raw data thought for model handling.\n",
    "                I. activities in text form paired with their classroom\n",
    "                II. All other features at that time stamp\n",
    "                III. target values\n",
    "        \"\"\"\n",
    "\n",
    "        #get row in df for data to be evaluated:\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        #First getting the \"all other data \" features tensor\n",
    "        #Date-time encoding tensor\n",
    "        enc_dt_tens = self.datetime_enc(row[\"date\"],row[\"time\"]) #date-time encoding\n",
    "\n",
    "        #weather data tensor\n",
    "        weather_tens = torch.tensor(row.drop([\"date\",\"time\",\"Q-Enginyeria (Cos Central) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Espina 4) [kWh] [Q-Enginyeria]\",\"Q-Enginyeria (Qu√≠mica) [kWh] [Q-Enginyeria]\"]))\n",
    "\n",
    "        #second get the target values tensor\n",
    "        target_tens = torch.tensor(row[self.target_labels])\n",
    "        \n",
    "        #Get activities at given time and date\n",
    "        activities = self.activty_class_perT(row[\"date\"],row[\"time\"])\n",
    "        if self.activitivity_encoding_mode <= 1:\n",
    "            one_hot = self.activity_class_one_hot(activities)\n",
    "            \n",
    "            if(self.activitivity_encoding_mode == 0):\n",
    "                one_hot = torch.tensor(one_hot)\n",
    "                #return values\n",
    "                sample = {'x': torch.cat((enc_dt_tens,weather_tens,one_hot),axis=0), 'y': target_tens}\n",
    "                return sample\n",
    "            \n",
    "            if(self.activitivity_encoding_mode == 1):\n",
    "                one_hot = one_hot.reshape(1, -1) #create extra dimension because this counts as only one sample\n",
    "                smaller_x = torch.tensor(self.ocu_one_hot_pca.transform(one_hot))[0] #get rid of extra dim\n",
    "                sample = {'x': torch.cat((enc_dt_tens,weather_tens,smaller_x),axis=0), 'y': target_tens}\n",
    "                return sample\n",
    "        \n",
    "        if self.activitivity_encoding_mode == 2:\n",
    "            #get bert embeddings\n",
    "            emb_activ = []\n",
    "            for i,actv in activities.iterrows(): #iterate found activities   \n",
    "                emb_activ.append(self.bert_embeddings[\"ocu_plus_space\"][\"mean_vect\"][actv[\"Activitat\"] + \" \" + actv[\"Espai\"]])\n",
    "            emb_activ = torch.tensor(np.array(emb_activ))\n",
    "\n",
    "            #get espai one-hot\n",
    "            espai_one_hot = self.class_one_hot(activities)\n",
    "\n",
    "            #padd with occupation 0 vector so all samples are same shape and espai with \"\"NO ESPAI\"\n",
    "            if emb_activ.shape[0] < self.max_ocu_lenght: \n",
    "                emb_activ = torch.cat((emb_activ,torch.zeros((self.max_ocu_lenght-emb_activ.shape[0],768))),axis=0)\n",
    "                espai_padd = pd.DataFrame(columns=[\"Espai\"]) \n",
    "                espai_padd[\"Espai\"] = [\"NO ESPAI\"]* (self.max_ocu_lenght-len(espai_one_hot))\n",
    "                padd_one_hot = self.class_one_hot(espai_padd)\n",
    "                espai_one_hot = torch.cat((espai_one_hot,padd_one_hot))\n",
    "                \n",
    "            sample = {'ocu_ber_emb': emb_activ,'espai_enc':espai_one_hot, \"general_data\":torch.cat((enc_dt_tens,weather_tens),axis=0), 'y': target_tens}\n",
    "            return sample\n",
    "        \n",
    "        if self.activitivity_encoding_mode == 2.5:\n",
    "            assert \"NOT IMPLEMENTED YET\"\n",
    "            pass\n",
    "            \n",
    "        if self.activitivity_encoding_mode == 3:\n",
    "            emb_activ = []\n",
    "            for i,actv in activities.iterrows(): #iterate found activities   \n",
    "                emb_activ.append(self.bert_embeddings[\"ocu_plus_space\"][\"mean_vect\"][actv[\"Activitat\"] + \" \" + actv[\"Espai\"]])\n",
    "            emb_activ = torch.tensor(np.array(emb_activ))\n",
    "\n",
    "            if emb_activ.shape[0] < self.max_ocu_lenght: #padd with occupation 0 vector so all samples are same shape\n",
    "                emb_activ = torch.cat((emb_activ,torch.zeros((self.max_ocu_lenght-emb_activ.shape[0],768))),axis=0)\n",
    "            \n",
    "            #return values\n",
    "            sample = {'activ': torch.tensor(emb_acti), 'general_data':torch.cat((enc_dt_tens,weather_tens),axis=0), 'y': target_tens}\n",
    "            return sample\n",
    "        \n",
    "        if self.activitivity_encoding_mode == 3.5:\n",
    "            assert \"NOT IMPLEMENTED YET\"\n",
    "            #errors to solve have to recalculate bert passing\n",
    "            emb_acti = []\n",
    "            for i,actv in activities.iterrows(): #iterate found activities   \n",
    "                emb_acti.append(self.bert_embeddings[\"ocu_plus_space\"][\"h_states\"][actv[\"Activitat\"] + \" \" + actv[\"Espai\"]])\n",
    "                print(self.bert_embeddings[\"ocu_plus_space\"][\"h_states\"][actv[\"Activitat\"] + \" \" + actv[\"Espai\"]].shape)\n",
    "            emb_acti = torch.tensor(emb_acti)\n",
    "            \n",
    "            print(emb_acti.shape)\n",
    "            #return values\n",
    "            return torch.tensor(emb_acti), torch.cat((enc_dt_tens,weather_tens),axis=0) , target_tens\n",
    "\n",
    "        if self.activitivity_encoding_mode == 4:\n",
    "            assert \"NOT IMPLEMENTED YET\"\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = energyProject_dataset(\"data/updated_merged_data.csv\",\"data/ocupacio_enginyeria_2022.csv\",\"data/bert_embedded.pkl\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset as pickle so we don't have to run init every time and it is only one file to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file and use dump()\n",
    "with open('data/dataset_class.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset class\n",
    "from dataset import energyProject_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset object file\n",
    "with (open('data/dataset_class.pkl', \"rb\")) as openfile:\n",
    "    dataset = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now you can create a dataloader and use it!\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the dataset is in 0 mode. Here are the modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the dataset mode just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.activitivity_encoding_mode = 2 #or any value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you don't even have to update the dataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch of data\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Print the keys of the batch dictionary\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "\n",
    "# Print the shape of each item in the batch\n",
    "for key in batch.keys():\n",
    "    print(f\"Shape of batch['{key}']:\", batch[key].shape)\n",
    "\n",
    "# Print the first few examples in the batch\n",
    "print(\"First few examples in the batch:\")\n",
    "for key in batch.keys():\n",
    "    print(f\"batch['{key}'][0]:\", batch[key][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU/LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch GRU model for regression tasks, which takes three inputs and predicts a continuous output.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of expected features in the input 'ocu_ber_emb' and 'espai_enc'\n",
    "        hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "        output_size (int): The number of expected features in the output.\n",
    "        num_layers (int): Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a `stacked GRU`.\n",
    "        general_data_size (int): The number of features in the 'general_data' input.\n",
    "\n",
    "    Inputs: ocu_ber_emb, espai_enc, general_data\n",
    "        - **ocu_ber_emb** of shape `(batch, seq_len, input_size - 1)`: tensor containing the features of the 'ocu_ber_emb' input sequence.\n",
    "          The input can also be a packed variable length sequence.\n",
    "        - **espai_enc** of shape `(batch, seq_len)`: tensor containing the features of the 'espai_enc' input sequence.\n",
    "        - **general_data** of shape `(batch, general_data_size)`: tensor containing the features of the 'general_data' input.\n",
    "\n",
    "    Outputs: output\n",
    "        - **output** of shape `(batch, output_size)`: tensor containing the output features from the last layer of the GRU, for each t.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, general_data_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        # GRU layer for 'ocu_ber_emb' and 'espai_enc'\n",
    "        self.gru = nn.GRU(input_size + 1, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer to reduce 'general_data' dimension\n",
    "        self.linear1 = nn.Linear(general_data_size, hidden_size)\n",
    "        \n",
    "        # Final linear layer for output\n",
    "        self.linear2 = nn.Linear(2 * hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, ocu_ber_emb, espai_enc, general_data):\n",
    "        # Expand 'espai_enc' dimensions to concatenate with 'ocu_ber_emb'\n",
    "        espai_enc = espai_enc.unsqueeze(2)\n",
    "        \n",
    "        # Concatenate 'ocu_ber_emb' and 'espai_enc'\n",
    "        x = torch.cat((ocu_ber_emb, espai_enc), dim=2)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        out, _ = self.gru(x)\n",
    "        \n",
    "        # Use only the final output of the GRU\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass 'general_data' through linear layer\n",
    "        general_data_out = self.linear1(general_data)\n",
    "        \n",
    "        # Concatenate GRU output and 'general_data' output\n",
    "        out = torch.cat((out, general_data_out), dim=1)\n",
    "        \n",
    "        # Final linear layer\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful stuff to get the model parameters\n",
    "\n",
    "# Initialize empty lists to hold ocu_codes and espai_codes\n",
    "ocu_codes = []\n",
    "espai_codes = []\n",
    "\n",
    "# Loop over the dataset\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    ocu_codes.append(sample['ocu_ber_emb'])\n",
    "    espai_codes.append(sample['espai_enc'])\n",
    "\n",
    "# Convert lists to pandas Series to easily count unique values\n",
    "ocu_codes = pd.Series(ocu_codes)\n",
    "espai_codes = pd.Series(espai_codes)\n",
    "\n",
    "n_ocu_codes = ocu_codes.nunique()\n",
    "n_espai_codes = espai_codes.nunique()\n",
    "\n",
    "print(f\"Number of unique ocu_codes: {n_ocu_codes}\")\n",
    "print(f\"Number of unique espai_codes: {n_espai_codes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_data_sizes = []\n",
    "\n",
    "# Loop over the dataset\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    general_data_sizes.append(len(sample['general_data']))\n",
    "\n",
    "# Convert list to pandas Series to easily get max value\n",
    "general_data_sizes = pd.Series(general_data_sizes)\n",
    "\n",
    "general_data_vector_size = general_data_sizes.max()\n",
    "\n",
    "print(f\"General data vector size: {general_data_vector_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocu_ber_emb = sample['ocu_ber_emb']\n",
    "print(ocu_ber_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 768  # Size of 'ocu_ber_emb' \n",
    "hidden_size = 256  # Hyperparameter, can be adjusted\n",
    "num_layers = 2  # Hyperparameter, can be adjusted\n",
    "output_size = 3  # Size of 'y'\n",
    "general_data_size = 21  # Size of 'general_data'\n",
    "\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size, num_layers, general_data_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the working version of the training loop, without the wandb.\n",
    "\n",
    "# # Instantiate the loss function and optimizer\n",
    "# criterion = nn.MSELoss()  # Change this if your task is not regression\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# # Number of epochs\n",
    "# num_epochs = 100\n",
    "\n",
    "# # Path to save model\n",
    "# model_save_path = 'gru_model.pt'\n",
    "\n",
    "# # Training loop\n",
    "# best_loss = float('inf')\n",
    "# for epoch in range(num_epochs):\n",
    "#     gru_model.train()  # Set the model to training mode\n",
    "#     epoch_losses = []\n",
    "#     progress_bar = tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#     for batch in progress_bar:\n",
    "#         # Move tensors to the correct device\n",
    "#         ocu_ber_emb = batch['ocu_ber_emb'].to(device).float()\n",
    "#         espai_enc = batch['espai_enc'].to(device).float()\n",
    "#         general_data = batch['general_data'].to(device).float()\n",
    "#         y = batch['y'].to(device).float()\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         output = model(ocu_ber_emb, espai_enc, general_data)\n",
    "#         # Compute loss\n",
    "#         loss = criterion(output, y)\n",
    "\n",
    "#         # Perform backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Perform optimization\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Record the loss\n",
    "#         epoch_losses.append(loss.item())\n",
    "\n",
    "#         # Update progress bar\n",
    "#         progress_bar.set_postfix({'loss': sum(epoch_losses) / len(epoch_losses)})\n",
    "\n",
    "#     # Compute the average loss for this epoch\n",
    "#     avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} Loss: {avg_loss}\")\n",
    "\n",
    "#     # Save the model if it has the best loss yet\n",
    "#     if avg_loss < best_loss:\n",
    "#         best_loss = avg_loss\n",
    "#         torch.save(gru_model.state_dict(), model_save_path)\n",
    "#         print(f\"Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_dict(original_dict):\n",
    "    nested_dict = {}\n",
    "    for key, value in original_dict.items():\n",
    "        parts = key.split(\".\")\n",
    "        d = nested_dict\n",
    "        for part in parts[:-1]:\n",
    "            if part not in d:\n",
    "                d[part] = {}\n",
    "            d = d[part]\n",
    "        d[parts[-1]] = value\n",
    "    return nested_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "def train(config: Dict = None):\n",
    "    # Weights & Biases Configuration\n",
    "    with wandb.init(config=config):\n",
    "        # unpack config dictionaries\n",
    "        config = wandb.config\n",
    "        config = nested_dict(config)\n",
    "        \n",
    "        optimizer_config = config[\"optimizer\"] #TODO find out why it needs to be like this and config['optimizer']['type'] doesn't work\n",
    "        \n",
    "        # Fixed model parameters\n",
    "        input_size = 768\n",
    "        output_size = 3\n",
    "        general_data_size = 21\n",
    "\n",
    "        # Instantiate the model using the configuration parameters\n",
    "        gru_model = GRUModel(input_size=input_size, \n",
    "                            hidden_size=config['hidden_size'], \n",
    "                            num_layers=config['num_layers'], \n",
    "                            output_size=output_size, \n",
    "                            general_data_size=general_data_size)\n",
    "        gru_model = gru_model.to(device)\n",
    "\n",
    "        # Instantiate the loss function\n",
    "        criterion = nn.MSELoss()  # Change this if your task is not regression\n",
    "\n",
    "        # Define the optimizer based on the configuration\n",
    "        if optimizer_config[\"type\"] == 'adam':\n",
    "            optimizer = torch.optim.Adam(gru_model.parameters(), lr = 0.01)\n",
    "        elif optimizer_config[\"type\"] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(gru_model.parameters(), lr = 0.01)\n",
    "\n",
    "        \n",
    "        # Number of epochs\n",
    "        num_epochs = 10\n",
    "\n",
    "        # Training loop\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(num_epochs):\n",
    "            gru_model.train()  # Set the model to training mode\n",
    "            epoch_losses = []\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            for batch in progress_bar:\n",
    "                # Move tensors to the correct device\n",
    "                ocu_ber_emb = batch['ocu_ber_emb'].to(device)\n",
    "                espai_enc = batch['espai_enc'].to(device)\n",
    "                general_data = batch['general_data'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                output = gru_model(ocu_ber_emb.float(), espai_enc.float(), general_data.float())\n",
    "                # Compute loss\n",
    "                loss = criterion(output, y.float())\n",
    "\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Record the loss\n",
    "                epoch_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "            # Compute the average loss for this epoch\n",
    "            avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "            wandb.log({'Epoch_Loss': avg_loss})  # Log the average loss for this epoch to Weights & Biases\n",
    "\n",
    "            # Save the model if it has the best loss yet\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(gru_model.state_dict(), 'gru_model.pt')\n",
    "                wandb.save('gru_model.pt')\n",
    "                print(f\"Model saved at {'gru_model.pt'}\")\n",
    "\n",
    "        # Close the Weights & Biases run\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as stream:\n",
    "    try:\n",
    "        sweep_config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"energy_project_uab\")\n",
    "wandb.agent(sweep_id, function=train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any other approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerations about using Weights and Biases for tracking the experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install the Weights & Biases (wandb) library\n",
    "\n",
    "If you haven't installed the wandb library yet, you can do so using pip. Run the following command in a new code cell:\n",
    "\n",
    "```!pip install wandb```\n",
    "\n",
    "#### 2. Import wandb into your script\n",
    "\n",
    "Add the following line at the beginning of your script to import the wandb library:\n",
    "\n",
    "\n",
    "```import wandb```\n",
    "\n",
    "#### 3. Login to your W&B account\n",
    "\n",
    "You need to login to your W&B account before you can start logging data. Run the following command in a new code cell:\n",
    "\n",
    "```!wandb login```\n",
    "\n",
    "You'll be asked for your API key, which you can get from the W&B website.\n",
    "\n",
    "#### 4. Initialize wandb in your script\n",
    "\n",
    "Now you can initialize wandb in your script. Replace \"my-project\" and \"run-name\" with your desired project and run names:\n",
    "\n",
    "```wandb.init(project=\"my-project\", name=\"run-name\")```\n",
    "\n",
    "#### 5. Log your model's hyperparameters\n",
    "\n",
    "You can log any of your model's hyperparameters using the wandb.config object:\n",
    "\n",
    "```\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "config.batch_size = 32\n",
    "config.epochs = 10\n",
    "... any other hyperparameters\n",
    "```\n",
    "\n",
    "#### 6. Log metrics like loss and accuracy\n",
    "\n",
    "To log metrics such as loss and accuracy, use the wandb.log method inside your training loop:\n",
    "\n",
    "```for epoch in range(config.epochs):\n",
    "    # training code here ...\n",
    "    wandb.log({\"loss\": loss, \"accuracy\": accuracy})\n",
    "```\n",
    "\n",
    "#### 7. Save your model weights with wandb\n",
    "\n",
    "```\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "wandb.save(\"model.pth\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
