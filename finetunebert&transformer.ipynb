{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset class\n",
    "from dataset import energyProject_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset object file\n",
    "with (open('data/dataset_class.pkl', \"rb\")) as openfile:\n",
    "    dataset = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now you can create a dataloader and use it!\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "dataset.activitivity_encoding_mode = 4 #or any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM_EMB = 768\n",
    "TIME_CLIMATE_DIM = 21\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepsmachine/miniforge3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,  DistilBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"[CLS] \"\n",
    "end_token = \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2tokens(txt):    \n",
    "    test_txt = start_token + txt + end_token\n",
    "    tokenized_text = tokenizer.tokenize(test_txt)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) \n",
    "    return indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, key_dim, val_dim, query_dim, hidden_dim, num_heads,FC_hidden=80):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.key_gen = nn.Sequential(\n",
    "            nn.Linear(key_dim, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.val_gen = nn.Sequential(\n",
    "            nn.Linear(val_dim, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.query_gen = nn.Sequential(\n",
    "            nn.Linear(query_dim, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, hidden_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self,keys,values,queries):\n",
    "        key = self.key_gen(keys) #generate key with FC key is directly the embedding\n",
    "        value = self.val_gen(values) #generate value with FC\n",
    "        query = self.query_gen(queries) #generate query with FC\n",
    "        output, _ =  self.multihead_attn(key=key, value=value, query=query)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attentive_model_pepemarti(hidden_dim=30,lstm_nl=1,target_dim=1,nheads=1,bert=bert)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, batch_size, num_epochs,lr=0.001,y_idx=0):\n",
    "    model.train()\n",
    "\n",
    "    criterion =  nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        tqdm_aux = tqdm(enumerate(dataloader),total=len(dataloader))\n",
    "        for batch, data in tqdm_aux:\n",
    "            ocu_emb, espai_enc, general_data = data[\"activ:\"] ,data[\"espai\"] ,data[\"general_data\"].float().to(device)\n",
    "            y = data[\"y\"][:,y_idx].float().to(device) #we'll do one counter for now\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #current batch size size\n",
    "            b_sz = len(ocu_emb[0])\n",
    "\n",
    "            #note the dataloader with a batch of 100 when reachs the end expects a batch of 60\n",
    "            h, c = model.init_hidden(b_sz) # Start with a new state in each batch            \n",
    "            h = h.to(device)\n",
    "            c = c.to(device)\n",
    "            y_pred, h,c= model(ocu_emb, espai_enc, general_data, h, c)\n",
    "            y_pred = y_pred[:,0,0]\n",
    "            loss = criterion(y_pred,y)  #cross entropy loss needs (N,C,seq_lenght)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #tqdm_aux.set_description(f\"Loss {loss.item()}\")\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "            losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/876 [00:00<?, ?it/s]ic| i: 4\n",
      "ic| l: 4\n",
      "ic| i: 9\n",
      "ic| l: 4\n",
      "ic| i: 14\n",
      "ic| l: 4\n",
      "ic| i: 19\n",
      "ic| l: 4\n",
      "ic| i: 24\n",
      "ic| l: 4\n",
      "ic| i: 29\n",
      "ic| l: 4\n",
      "ic| i: 34\n",
      "ic| l: 4\n",
      "ic| i: 39\n",
      "ic| l: 4\n",
      "ic| i: 44\n",
      "ic| l: 4\n",
      "ic| i: 49\n",
      "ic| l: 4\n",
      "ic| i: 4\n",
      "ic| l: 4\n",
      "ic| i: 9\n",
      "ic| l: 4\n",
      "ic| i: 14\n",
      "ic| l: 4\n",
      "ic| i: 19\n",
      "ic| l: 4\n",
      "ic| i: 24\n",
      "ic| l: 4\n",
      "ic| i: 29\n",
      "ic| l: 4\n",
      "ic| i: 34\n",
      "ic| l: 4\n",
      "ic| i: 39\n",
      "ic| l: 4\n",
      "ic| i: 44\n",
      "ic| l: 4\n",
      "ic| i: 49\n",
      "ic| l: 4\n",
      "ic| i: 4\n",
      "ic| l: 4\n",
      "ic| i: 9\n",
      "ic| l: 4\n",
      "ic| i: 14\n",
      "ic| l: 4\n",
      "ic| i: 19\n",
      "ic| l: 4\n",
      "ic| i: 24\n",
      "ic| l: 4\n",
      "ic| i: 29\n",
      "ic| l: 4\n",
      "ic| i: 34\n",
      "ic| l: 4\n",
      "ic| i: 39\n",
      "ic| l: 4\n",
      "ic| i: 44\n",
      "ic| l: 4\n",
      "ic| i: 49\n",
      "ic| l: 4\n",
      "ic| i: 4\n",
      "ic| l: 4\n",
      "ic| i: 9\n",
      "ic| l: 4\n",
      "ic| i: 14\n",
      "ic| l: 4\n",
      "ic| i: 19\n",
      "ic| l: 4\n",
      "ic| i: 24\n",
      "ic| l: 4\n",
      "ic| i: 29\n",
      "ic| l: 4\n",
      "ic| i: 34\n",
      "ic| l: 4\n",
      "ic| i: 39\n",
      "ic| l: 4\n",
      "ic| i: 44\n",
      "ic| l: 4\n",
      "ic| i: 49\n",
      "ic| l: 4\n",
      "ic| i: 4\n",
      "ic| l: 4\n",
      "ic| i: 9\n",
      "ic| l: 4\n",
      "ic| i: 14\n",
      "ic| l: 4\n",
      "ic| i: 19\n",
      "ic| l: 4\n",
      "ic| i: 24\n",
      "ic| l: 4\n",
      "ic| i: 29\n",
      "ic| l: 4\n",
      "ic| i: 34\n",
      "ic| l: 4\n",
      "ic| i: 39\n",
      "  0%|          | 0/876 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 2\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43my_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, batch_size, num_epochs, lr, y_idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m y_pred, h,c\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocu_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mespai_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneral_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred[:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred,y)  \u001b[38;5;66;03m#cross entropy loss needs (N,C,seq_lenght)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 66\u001b[0m, in \u001b[0;36mattentive_model_pepemarti.forward\u001b[0;34m(self, ocu_ber_emb, espai_enc, general_data, h, c)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tok_idx,tok_lens):\n\u001b[1;32m     65\u001b[0m     ic(i)\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mic\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     hidd \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m,i\u001b[38;5;241m-\u001b[39ml:i,:] \u001b[38;5;66;03m#create batch dimension\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     hidd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(hidd,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/icecream/icecream.py:208\u001b[0m, in \u001b[0;36mIceCreamDebugger.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    206\u001b[0m callFrame \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mcurrentframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format(callFrame, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m NoSourceAvailableError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    210\u001b[0m     prefix \u001b[39m=\u001b[39m callOrValue(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefix)\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/icecream/icecream.py:235\u001b[0m, in \u001b[0;36mIceCreamDebugger._format\u001b[0;34m(self, callFrame, *args)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m callNode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m NoSourceAvailableError()\n\u001b[0;32m--> 235\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_formatContext(callFrame, callNode)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    237\u001b[0m     time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_formatTime()\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/icecream/icecream.py:317\u001b[0m, in \u001b[0;36mIceCreamDebugger._formatContext\u001b[0;34m(self, callFrame, callNode)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_formatContext\u001b[39m(\u001b[39mself\u001b[39m, callFrame, callNode):\n\u001b[0;32m--> 317\u001b[0m     filename, lineNumber, parentFunction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getContext(\n\u001b[1;32m    318\u001b[0m         callFrame, callNode)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m parentFunction \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<module>\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    321\u001b[0m         parentFunction \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m parentFunction\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/icecream/icecream.py:333\u001b[0m, in \u001b[0;36mIceCreamDebugger._getContext\u001b[0;34m(self, callFrame, callNode)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getContext\u001b[39m(\u001b[39mself\u001b[39m, callFrame, callNode):\n\u001b[1;32m    332\u001b[0m     lineNumber \u001b[39m=\u001b[39m callNode\u001b[39m.\u001b[39mlineno\n\u001b[0;32m--> 333\u001b[0m     frameInfo \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetframeinfo(callFrame)\n\u001b[1;32m    334\u001b[0m     parentFunction \u001b[39m=\u001b[39m frameInfo\u001b[39m.\u001b[39mfunction\n\u001b[1;32m    336\u001b[0m     filepath \u001b[39m=\u001b[39m (realpath \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontextAbsPath \u001b[39melse\u001b[39;00m basename)(frameInfo\u001b[39m.\u001b[39mfilename)\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/inspect.py:1628\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isframe(frame):\n\u001b[1;32m   1626\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a frame or traceback object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(frame))\n\u001b[0;32m-> 1628\u001b[0m filename \u001b[39m=\u001b[39m getsourcefile(frame) \u001b[39mor\u001b[39;00m getfile(frame)\n\u001b[1;32m   1629\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1630\u001b[0m     start \u001b[39m=\u001b[39m lineno \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m context\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/inspect.py:829\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[1;32m    828\u001b[0m \u001b[39m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m module \u001b[39m=\u001b[39m getmodule(\u001b[39mobject\u001b[39;49m, filename)\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, \u001b[39m'\u001b[39m\u001b[39m__loader__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/inspect.py:871\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m ismodule(module) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(module, \u001b[39m'\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    870\u001b[0m     f \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n\u001b[0;32m--> 871\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39m==\u001b[39m _filesbymodname\u001b[39m.\u001b[39;49mget(modname, \u001b[39mNone\u001b[39;49;00m):\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Have already mapped this module, so skip it\u001b[39;00m\n\u001b[1;32m    873\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     _filesbymodname[modname] \u001b[39m=\u001b[39m f\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "losses = train(dataloader, model, batch_size=batch_size, num_epochs=num_epochs,lr=0.0001,y_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcolumn_scalers[dataset\u001b[38;5;241m.\u001b[39mtarget_labels[\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#get the simple loss back from the squared error\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m scaled_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(np\u001b[38;5;241m.\u001b[39mpower(dataset\u001b[38;5;241m.\u001b[39mdenormalize_values(np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mlosses\u001b[49m),scaler),\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(scaled_losses[\u001b[38;5;241m5\u001b[39m:])\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel L2 loss without normalisation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scaler = dataset.column_scalers[dataset.target_labels[2]]\n",
    "#get the simple loss back from the squared error\n",
    "scaled_losses = np.ravel(np.power(dataset.denormalize_values(np.sqrt(losses),scaler),2))\n",
    "plt.plot(scaled_losses[5:])\n",
    "plt.title('model L2 loss without normalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_consumption(model,dataloader,y_idx):\n",
    "    real = []\n",
    "    pred = []\n",
    "    h, c = model.init_hidden(dataloader.batch_size) # Start with a new state in each batch            \n",
    "    h = h.to(device)\n",
    "    c = c.to(device)\n",
    "    for batch, data in tqdm(enumerate(dataloader),total=len(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            ocu_emb, espai_enc, general_data = data[\"activ:\"] ,data[\"espai\"] ,data[\"general_data\"].float().to(device)\n",
    "            y = data[\"y\"][:,y_idx].float().to(device) #we'll do one counter for now\n",
    "\n",
    "            #current batch size size\n",
    "            b_sz = len(ocu_emb[0])\n",
    "\n",
    "            #note the dataloader with a batch of 100 when reachs the end expects a batch of 60\n",
    "            h, c = model.init_hidden(b_sz) # Start with a new state in each batch            \n",
    "            h = h.to(device)\n",
    "            c = c.to(device)\n",
    "            y_pred, h,c= model(ocu_emb, espai_enc, general_data, h, c)\n",
    "            y_pred = y_pred[:,0,0]\n",
    "            pred += list(y_pred.to(\"cpu\"))\n",
    "            real += list(y.to(\"cpu\"))\n",
    "    return real ,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/292 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 15/292 [00:52<16:17,  3.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m real, pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_consumption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m real \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m real]\n\u001b[1;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pred]\n",
      "Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mpredict_consumption\u001b[0;34m(model, dataloader, y_idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m y_pred, h,c\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocu_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mespai_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneral_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred[:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 78\u001b[0m, in \u001b[0;36mattentive_model_pepemarti.forward\u001b[0;34m(self, ocu_ber_emb, espai_enc, general_data, h, c)\u001b[0m\n\u001b[1;32m     75\u001b[0m     tok_idx\u001b[38;5;241m.\u001b[39mappend(tok\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#ic(tok_idx)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m btch_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#the last end token is three tokens. Get the hidden state of the last layer for the -4 token.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#btch_ocu =\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    590\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    361\u001b[0m )\n\u001b[1;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:222\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39;49mmasked_fill(\n\u001b[1;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39;49mtensor(torch\u001b[39m.\u001b[39;49mfinfo(scores\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mmin)\n\u001b[1;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(weights)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "real, pred = predict_consumption(model,dataloader,y_idx=2)\n",
    "real = [v.item() for v in real]\n",
    "pred = [v.item() for v in pred]\n",
    "scaler = dataset.column_scalers[dataset.target_labels[0]]\n",
    "real_sc = dataset.denormalize_values(real,scaler)\n",
    "pred_sc = dataset.denormalize_values(pred,scaler)\n",
    "plt.figure(figsize=(30, 6))  # Increase the figure size to make it larger in the x-axis\n",
    "\n",
    "plt.plot(real_sc, label='Real')  # Add a label for the real data\n",
    "plt.plot(pred_sc, label='Predicted')  # Add a label for the predicted data\n",
    "\n",
    "plt.legend()  # Add a legend to the plot\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
